# ðŸ“Š Daily Stock Fetch & Train Service

This microservice fetches daily OHLCV stock data from Polygon.io, stores it in BigQuery, trains LSTM models on historical prices, and uploads the models to Amazon S3 for use by downstream prediction services.

---

## ðŸš€ Features

- âœ… Fetches latest stock data for tracked tickers
- âœ… Inserts new data into BigQuery (deduplicated)
- âœ… Trains LSTM neural networks using TensorFlow
- âœ… Saves `.keras` models to disk
- âœ… Uploads trained models to S3 (versioned by date)

---

## âš™ï¸ Tech Stack

- **Python 3.10+**
- **TensorFlow / Keras**
- **Pandas, NumPy, Scikit-learn**
- **Google BigQuery** (data storage)
- **Amazon S3** (model storage)
- **Polygon.io API** (stock data source)
- **boto3** (AWS SDK)
- **cron / PM2** (optional scheduling)

---

## ðŸ“‚ Project Structure

```
daily-stock-service/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.py                # Main script (fetch + train)
â”‚   â”œâ”€â”€ config.py             # Loads creds from AWS SSM
â”‚   â”œâ”€â”€ daily_fetcher.py      # Polygon fetch + BigQuery insert
â”‚   â”œâ”€â”€ model_trainer.py      # LSTM training + S3 upload
â”‚   â””â”€â”€ utils/                # Helper functions (optional)
â”œâ”€â”€ models/                   # Saved .keras files (before upload)
â””â”€â”€ README.md
```

---

## ðŸ§ª Running the Service

### â–¶ï¸ Fetch + Train (full run)

```bash

python3 src/app.py
```

This will:
1. Fetch daily OHLCV data for each symbol
2. Insert data to BigQuery if not already present
3. Train LSTM model on full historical data
4. Upload model to `s3://brown-oak-stock-prediction-models/{YYYY-MM-DD}/{SYMBOL}_model.keras`

---

## ðŸ—ƒï¸ Data Flow

```
+-------------+         +--------------+         +------------+
|  Polygon.io | --->    |  BigQuery    | --->    |  Train LSTM|
+-------------+         +--------------+         +------------+
                                                         |
                                                         v
                                                +------------------+
                                                | Upload to S3 (.keras)
                                                +------------------+
```

---

## ðŸŒ Environment Requirements

- `GOOGLE_BIG_QUERY` (SSM secret path)
- `POLYGON_KEY` (SSM secret path)
- BigQuery dataset + table already created
- S3 bucket created (e.g. `brown-oak-stock-prediction-models`)
- IAM permissions for:
    - `ssm:GetParameter`
    - `s3:PutObject`
    - `s3:ListBucket`
    - `bigquery.jobs.create`, `bigquery.tables.getData`

---

## ðŸ“¦ Sample Output

```bash

â–¶ï¸ Processing AAPL...
âœ… Inserted 1 rows for AAPL on 2025-04-19 into stock_data
âœ… Model trained for AAPL
ðŸ“¤ Uploaded to s3://brown-oak-stock-prediction-models/2025-04-20/AAPL_lstm_model.keras
```

---

## ðŸ“… Suggested Cron Schedule (daily @ 6pm)

```cron
0 18 * * * cd /path/to/service && python3 src/app.py >> logs/fetch-train.log 2>&1
```

Or use PM2 for managed restarts.

---

## ðŸ§  Future Improvements

- [ ] Add Slack/email alerts on failure
- [ ] Automatically evaluate model performance
- [ ] Cache and reuse training artifacts
- [ ] Add TensorBoard logs

---

## ðŸ‘¤ Author

**Nguyen Bui**  
[GitHub](https://github.com/Jackbui96) â€¢ [LinkedIn](https://www.linkedin.com/in/jackbui96)

---

## ðŸ“˜ License

MIT
